{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arjun Garg\\Desktop\\NBA Play-by-Play\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Model, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('tensor.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = []\n",
    "current_game = []\n",
    "current_quarter = []\n",
    "\n",
    "for i, row in enumerate(data):\n",
    "    if np.array_equal(row, [0, 1, 0, 0, 0]):\n",
    "        current_quarter.append(row)\n",
    "        current_game.append(np.array(current_quarter))\n",
    "        current_quarter = []\n",
    "    if np.all(row == 0):\n",
    "        games.append(current_game)\n",
    "        current_game = []\n",
    "    else:\n",
    "        current_quarter.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_quarters = []\n",
    "TARGET_LENGTH = 705\n",
    "\n",
    "for game in games:\n",
    "    for quarter in game:\n",
    "        current_quarter = np.array(quarter).flatten()\n",
    "\n",
    "        if len(current_quarter) < TARGET_LENGTH:\n",
    "            padded = np.pad(current_quarter, (0, TARGET_LENGTH - len(current_quarter)), mode='constant', constant_values=0)\n",
    "            \n",
    "        flattened_quarters.append(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuarterDataset(Dataset):\n",
    "    def __init__(self, flattened_quarters, pad_token=[0, 0, 0, 0, 0]):\n",
    "        self.flattened_quarters = flattened_quarters\n",
    "        self.pad_token = pad_token\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.flattened_quarters)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = torch.tensor(self.flattened_quarters[idx], dtype=torch.long)\n",
    "        plays = sequence.view(-1, 5)\n",
    "        targets = plays.roll(-1, dims=0)\n",
    "        targets[-1] = torch.tensor(self.pad_token)\n",
    "        attention_mask = ~(plays == torch.tensor(self.pad_token)).all(dim=1).long()\n",
    "        return plays, targets, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_sizes, embedding_dims, embedding_output_dim):\n",
    "        super(TokenEmbeddings, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_sizes[0], embedding_dims[0]),\n",
    "            nn.Embedding(vocab_sizes[1], embedding_dims[1]),\n",
    "            nn.Embedding(vocab_sizes[2], embedding_dims[2]),\n",
    "            nn.Embedding(vocab_sizes[3], embedding_dims[3]),\n",
    "        ])\n",
    "        self.embeddings.append(self.embeddings[2])\n",
    "\n",
    "        self.projection = nn.Linear(sum(embedding_dims), embedding_output_dim)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens: (batch_size, sequence_length, 5)\n",
    "        embedded = [\n",
    "            self.embeddings[i](tokens[..., i]) for i in range(len(self.embeddings))\n",
    "        ]\n",
    "\n",
    "        concat_embeddings = torch.cat(embedded, dim=-1) # (batch_size, sequence_length, sum(embedding_dims))\n",
    "        projected_embedding = self.projection(concat_embeddings) # (batch_size, sequence_length, embedding_output_dim)\n",
    "        return projected_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComponentSubModelFF(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, context_embedding, local_context):\n",
    "        if local_context is None or local_context.size(1) == 0:\n",
    "            combined_input = context_embedding\n",
    "        else:\n",
    "            combined_input = torch.cat([context_embedding, local_context], dim=-1)\n",
    "        return self.layers(combined_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_components = [2, 3]\n",
    "shots = [1, 2, 3, 4]\n",
    "rebounds = [5, 6]\n",
    "\n",
    "class PlaySubModel(torch.nn.Module):\n",
    "    def __init__(self, context_embedding_dim, local_embedding_dims, hidden_dims, output_dims, token_embeddings):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = token_embeddings\n",
    "        \n",
    "        self.submodels = torch.nn.ModuleList([\n",
    "            ComponentSubModelFF(context_embedding_dim + sum(local_embedding_dims[:i]), hidden_dim, output_dim) for i, (hidden_dim, output_dim) in enumerate(zip(hidden_dims, output_dims))\n",
    "        ])\n",
    "    \n",
    "    def forward(self, context_embedding):\n",
    "        batch_size = context_embedding.size(0)\n",
    "        play_context = torch.zeros(batch_size, 0, device=context_embedding.device)\n",
    "        generated_play = []\n",
    "\n",
    "        for component_idx, submodel in enumerate(self.submodels):\n",
    "            logits = submodel(context_embedding, play_context)  # Filter the batch\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            generated_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "            token_embedding = self.token_embeddings.embeddings[component_idx](generated_token)\n",
    "            play_context = torch.cat([play_context, token_embedding], dim=-1)\n",
    "\n",
    "            generated_play.append(generated_token)\n",
    "        \n",
    "        return torch.stack(generated_play, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayAttentionTransformer(nn.Module):\n",
    "    def __init__(self, play_embedding_dim, transformer_hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = GPT2Config(\n",
    "            n_embd=transformer_hidden_dim,\n",
    "            n_layer=num_layers,\n",
    "            n_head=8,\n",
    "            n_positions=141\n",
    "        )\n",
    "\n",
    "        self.transformer = GPT2Model(self.config)\n",
    "        self.context_projection = nn.Linear(transformer_hidden_dim, play_embedding_dim)\n",
    "    \n",
    "    def forward(self, play_embeddings, attention_mask=None):\n",
    "        print(f\"play_embeddings shape: {play_embeddings.shape}\")  # Expected: [batch_size, seq_length, embed_dim]\n",
    "        print(f\"attention_mask shape: {attention_mask.shape}\")    # Expected: [batch_size, seq_length]\n",
    "        transformer_outputs = self.transformer(\n",
    "            inputs_embeds=play_embeddings, attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        hidden_states = transformer_outputs.last_hidden_state # (batch_size, num_plays, transformer_hidden_dim)\n",
    "        context_embedding = self.context_projection(hidden_states) # (batch_size, play_embedding_dim)\n",
    "        return context_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBAAutoregressiveModel(nn.Module):\n",
    "    def __init__(self, vocab_sizes, embedding_dims, play_embedding_dim, transformer_hidden_dim, num_layers, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = TokenEmbeddings(vocab_sizes, embedding_dims, play_embedding_dim)\n",
    "        self.play_transformer = PlayAttentionTransformer(play_embedding_dim, transformer_hidden_dim, num_layers)\n",
    "        self.play_submodel = PlaySubModel(play_embedding_dim, embedding_dims, hidden_dims, vocab_sizes, self.token_embeddings)\n",
    "    \n",
    "    def forward(self, tokens, attention_mask=None):\n",
    "        play_embeddings = self.token_embeddings(tokens) # (batch_size, num_plays, play_embedding_dim)\n",
    "        context_embedding = self.play_transformer(play_embeddings, attention_mask) # (batch_size, play_embedding_dim)\n",
    "        next_play = self.play_submodel(context_embedding) # (batch_size, 5)\n",
    "        return next_play\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sizes = [14, 2194, 9, 3, 2194]\n",
    "embedding_dims = [8, 32, 4, 1, 32]\n",
    "play_embedding_dim = 64\n",
    "transformer_hidden_dim = 64\n",
    "num_layers = 4\n",
    "hidden_dims = [32, 32, 16, 16, 32]\n",
    "\n",
    "model = NBAAutoregressiveModel(vocab_sizes, embedding_dims, play_embedding_dim, transformer_hidden_dim, num_layers, hidden_dims).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "dataset = QuarterDataset(flattened_quarters)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play_embeddings shape: torch.Size([16, 64])\n",
      "attention_mask shape: torch.Size([16, 5])\n",
      "play_embeddings shape: torch.Size([16, 64])\n",
      "attention_mask shape: torch.Size([16, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (16) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[198], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m cur_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((plays\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m5\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Match cur_context size\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(plays\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m---> 14\u001b[0m     play_outputs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_attention_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     15\u001b[0m     cur_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([cur_context[:, \u001b[38;5;241m1\u001b[39m:], plays[:, i]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m     cur_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([cur_attention_mask[:, \u001b[38;5;241m1\u001b[39m:], attention_mask[:, i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Arjun Garg\\Desktop\\NBA Play-by-Play\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arjun Garg\\Desktop\\NBA Play-by-Play\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[194], line 10\u001b[0m, in \u001b[0;36mNBAAutoregressiveModel.forward\u001b[1;34m(self, tokens, attention_mask)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      9\u001b[0m     play_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embeddings(tokens) \u001b[38;5;66;03m# (batch_size, num_plays, play_embedding_dim)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     context_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplay_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (batch_size, play_embedding_dim)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     next_play \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplay_submodel(context_embedding) \u001b[38;5;66;03m# (batch_size, 5)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_play\n",
      "File \u001b[1;32mc:\\Users\\Arjun Garg\\Desktop\\NBA Play-by-Play\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arjun Garg\\Desktop\\NBA Play-by-Play\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[193], line 18\u001b[0m, in \u001b[0;36mPlayAttentionTransformer.forward\u001b[1;34m(self, play_embeddings, attention_mask)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplay_embeddings shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplay_embeddings\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Expected: [batch_size, seq_length, embed_dim]\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattention_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)    \u001b[38;5;66;03m# Expected: [batch_size, seq_length]\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplay_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state \u001b[38;5;66;03m# (batch_size, num_plays, transformer_hidden_dim)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m context_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_projection(hidden_states) \u001b[38;5;66;03m# (batch_size, play_embedding_dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Arjun Garg\\Desktop\\NBA Play-by-Play\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arjun Garg\\Desktop\\NBA Play-by-Play\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Arjun Garg\\Desktop\\NBA Play-by-Play\\myenv\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1040\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _use_sdpa:\n\u001b[1;32m-> 1040\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_4d_causal_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1048\u001b[0m         \u001b[38;5;66;03m# We create a 3D attention mask from a 2D tensor mask.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m         \u001b[38;5;66;03m# Sizes are [batch_size, 1, 1, to_seq_length]\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;66;03m# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m         \u001b[38;5;66;03m# this attention mask is more simple than the triangular masking of causal attention\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m         \u001b[38;5;66;03m# used in OpenAI GPT, we just need to prepare the broadcast dimension here.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Arjun Garg\\Desktop\\NBA Play-by-Play\\myenv\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:391\u001b[0m, in \u001b[0;36m_prepare_4d_causal_attention_mask_for_sdpa\u001b[1;34m(attention_mask, input_shape, inputs_embeds, past_key_values_length, sliding_window)\u001b[0m\n\u001b[0;32m    389\u001b[0m     expanded_4d_mask \u001b[38;5;241m=\u001b[39m attention_mask\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     expanded_4d_mask \u001b[38;5;241m=\u001b[39m \u001b[43mattn_mask_converter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_4d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;66;03m# Attend to all tokens in masked rows from the causal_mask, for example the relevant first rows when\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;66;03m# using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;66;03m# Details: https://github.com/pytorch/pytorch/issues/110213\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing \u001b[38;5;129;01mand\u001b[39;00m expanded_4d_mask\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Arjun Garg\\Desktop\\NBA Play-by-Play\\myenv\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:139\u001b[0m, in \u001b[0;36mAttentionMaskConverter.to_4d\u001b[1;34m(self, attention_mask_2d, query_length, dtype, key_value_length)\u001b[0m\n\u001b[0;32m    134\u001b[0m expanded_attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_mask(attention_mask_2d, dtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m    135\u001b[0m     attention_mask_2d\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    136\u001b[0m )\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m causal_4d_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 139\u001b[0m     expanded_attn_mask \u001b[38;5;241m=\u001b[39m \u001b[43mcausal_4d_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_attn_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# expanded_attn_mask + causal_4d_mask can cause some overflow\u001b[39;00m\n\u001b[0;32m    142\u001b[0m expanded_4d_mask \u001b[38;5;241m=\u001b[39m expanded_attn_mask\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (16) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (plays, targets, attention_mask) in enumerate(dataloader):\n",
    "        plays, targets, attention_mask = plays.to(device), targets.to(device), attention_mask.to(device)\n",
    "\n",
    "        play_outputs = []\n",
    "        cur_context = torch.zeros((batch_size, 5), dtype=torch.long).to(device)\n",
    "        cur_attention_mask = torch.ones((plays.size(0), 5), dtype=torch.long).to(device)  # Match cur_context size\n",
    "\n",
    "        for i in range(plays.size(1)):\n",
    "            play_outputs.append(model(cur_context, cur_attention_mask))\n",
    "            cur_context = torch.cat([cur_context[:, 1:], plays[:, i]], dim=1)\n",
    "            cur_attention_mask = torch.cat([cur_attention_mask[:, 1:], attention_mask[:, i].unsqueeze(1)], dim=1)\n",
    "\n",
    "        play_outputs = torch.stack(play_outputs, dim=1)\n",
    "\n",
    "        loss = 0\n",
    "        for i in range(5):\n",
    "            loss += criterion(play_outputs[i].view(-1, play_outputs[i].size(-1)), targets[:, :, i].view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
